{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;'> Input Details </p>\n",
    "\n",
    "- All end dates (Train, Test and Forecast)\n",
    "- Specify date format\n",
    "- Provide data file name (file should be in csv format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, itertools, logging\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from prophet import Prophet\n",
    "from tbats import TBATS\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logger = logging.getLogger('cmdstanpy')\n",
    "logger.addHandler(logging.NullHandler())\n",
    "logger.propagate = False\n",
    "logger.setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainStartDate = datetime(2018,1,1)\n",
    "trainEndDate = datetime(2022,1,1)\n",
    "testStartDate = datetime(2022,2,1)\n",
    "testEndDate = datetime(2023,1,1)\n",
    "forecastStartDate = datetime(2023, 2, 1)\n",
    "forecastEndDate = datetime(2024, 2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary functions for data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection\n",
    "def detect_outlier(x):\n",
    "    Q1 = np.quantile(x, 0.25)\n",
    "    Q3 = np.quantile(x, 0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    return (x < Q1 - (1.5*IQR)) | (x > Q3 + (1.5*IQR))\n",
    "# Capping outlier value\n",
    "def cap_outliers(x):\n",
    "    Q1 = np.quantile(x, 0.25)\n",
    "    Q3 = np.quantile(x, 0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_limit = Q1 - (1.5*IQR)\n",
    "    upper_limit = Q3 + (1.5*IQR)\n",
    "    x_cap = np.where(x > upper_limit, upper_limit, np.where(x < lower_limit, lower_limit, x))\n",
    "    return x_cap\n",
    "# MAPE (Mean Absolute Percentage Error)\n",
    "def MAPE(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "# Accuracy\n",
    "def Accuracy(mape):\n",
    "    return 100.0 - mape\n",
    "# Root Mean Square Error\n",
    "def RMSE(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.sqrt(np.mean((y_true - y_pred)**2))\n",
    "\n",
    "# Making function for data processing\n",
    "# ----------------------------------------------------------------------------\n",
    "def data_part(df, part):\n",
    "    Date = pd.date_range(trainStartDate - timedelta(days=1), testEndDate, freq = 'M') + timedelta(days = 1)\n",
    "    P1 = df[df['Part'] == part].reset_index(drop=True)\n",
    "    P2 = P1.groupby(['Date'])['Quantity'].sum().reset_index()\n",
    "    P2.set_index('Date', inplace = True)\n",
    "    temp_df = pd.DataFrame(P2, index = Date)\n",
    "    temp_df['Flag NA'] = temp_df['Quantity'].apply(lambda x: np.isnan(x))\n",
    "    temp_df = temp_df[temp_df[temp_df['Flag NA'] == False].index[0]:]\n",
    "    temp_df.fillna(method = 'ffill', inplace = True)\n",
    "    temp_df['Outlier'] = detect_outlier(temp_df['Quantity'])\n",
    "    temp_df['Quantity'] = cap_outliers(temp_df['Quantity'].values)\n",
    "    return temp_df\n",
    "# ADF test\n",
    "def adf_test(series):\n",
    "    result = adfuller(series)\n",
    "    if result[1] < 0.05:\n",
    "        d = 0\n",
    "    else:\n",
    "        d = 1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================================================\n",
    "#                                        Halt Winter\n",
    "# -----------------------------------------------------------------------------------------------------------\n",
    "def RUN_HW(df, part):\n",
    "    best_rmse = float(\"inf\")\n",
    "    best_comb = None\n",
    "    best_abg = None\n",
    "    best_mape = None\n",
    "    best_acc = None\n",
    "    best_pred = None\n",
    "\n",
    "    # Getting data for each group\n",
    "    demo_data = data_part(df, part)\n",
    "    # Data splitting\n",
    "    train_data = demo_data[:trainEndDate]\n",
    "    test_data = demo_data[trainEndDate + timedelta(days = 1):testEndDate]\n",
    "    forecast_dates = pd.date_range(testEndDate, forecastEndDate, freq = 'M') + timedelta(days = 1)\n",
    "    n = len(test_data)\n",
    "    m = len(forecast_dates)\n",
    "    y_true = test_data['Quantity']\n",
    "    # Making all possible combination\n",
    "    values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    all_combination = list(itertools.product(['add', 'mul'], ['add', 'mul']))\n",
    "    abg_combination = list(itertools.product(values, values, values))\n",
    "\n",
    "    for comb in tqdm(all_combination, desc = f'\\nPart {part}', leave = False):\n",
    "        for abg in tqdm(abg_combination, desc = 'Searching for best ABG ', leave = False):\n",
    "            alpha, beta, gamma = abg\n",
    "            try:\n",
    "                hw_model = ExponentialSmoothing(train_data[\"Quantity\"], trend=comb[0], seasonal=comb[1], seasonal_periods=12,damped = False)\n",
    "                model = hw_model.fit(smoothing_level=alpha, smoothing_slope=beta, smoothing_seasonal=gamma)\n",
    "                y_pred = model.predict(start=test_data.index[0], end=test_data.index[-1])\n",
    "                mape = MAPE(y_true, y_pred)\n",
    "                acc = Accuracy(mape)\n",
    "                rmse = RMSE(y_true, y_pred)\n",
    "                if rmse < best_rmse:\n",
    "                    best_rmse = rmse\n",
    "                    best_comb = comb\n",
    "                    best_abg = abg\n",
    "                    best_mape = mape\n",
    "                    best_acc = acc\n",
    "                    best_pred = y_pred\n",
    "            except:\n",
    "                pass\n",
    "    hw_final_model = ExponentialSmoothing(train_data[\"Quantity\"], trend=best_comb[0], seasonal=best_comb[1], seasonal_periods=12,damped = False)\n",
    "    final_model = hw_final_model.fit(smoothing_level = best_abg[0], smoothing_slope = best_abg[1], smoothing_seasonal = best_abg[2])\n",
    "    forecast = final_model.predict(start = testEndDate+timedelta(days=1), end = forecastEndDate)\n",
    "    temp = pd.DataFrame({'Part':np.repeat(part,n), \n",
    "                         'Date':test_data.index.values, \n",
    "                         'Original':test_data.Quantity.values, \n",
    "                         'Prediction':best_pred.values, \n",
    "                         'MAPE':np.repeat(best_mape,n), \n",
    "                         'RMSE':np.repeat(best_rmse,n), \n",
    "                         'Accuracy':np.repeat(best_acc,n)})\n",
    "    temp.insert(6, 'Order', [(best_comb, best_abg) for i in range(n)])\n",
    "    forecast_df = pd.DataFrame({'Part':np.repeat(part,m), \n",
    "                                'Date':forecast_dates.values, \n",
    "                                'Forecast':forecast.values})\n",
    "    forecast_df.insert(3, 'Order', [(best_comb, best_abg) for i in range(m)])\n",
    "    return temp, forecast_df\n",
    "\n",
    "# ===========================================================================================================\n",
    "#                          Auto Regressive Moving Average (ARMA)\n",
    "# -----------------------------------------------------------------------------------------------------------\n",
    "def RUN_ARMA(df, part):\n",
    "    best_rmse = float(\"inf\")\n",
    "    best_order = None\n",
    "    best_mape = None\n",
    "    best_acc = None\n",
    "    best_pred = None\n",
    "\n",
    "    # Getting data for each group\n",
    "    demo_data = data_part(df, part)\n",
    "\n",
    "    # Data splitting\n",
    "    train_data = demo_data[:trainEndDate]\n",
    "    test_data = demo_data[trainEndDate + timedelta(days = 1):testEndDate]\n",
    "    forecast_dates = pd.date_range(testEndDate, forecastEndDate, freq = 'M') + timedelta(days = 1)\n",
    "    n = len(test_data)\n",
    "    m = len(forecast_dates)\n",
    "    y_true = test_data['Quantity']\n",
    "    \n",
    "    p = range(0, 12)\n",
    "    d = [0]\n",
    "    q = range(0, 12)\n",
    "    pdq_combination = list(itertools.product(p,d,q))\n",
    "    \n",
    "    for order in tqdm(pdq_combination, desc = f'\\nPart: {part}', leave = False):\n",
    "        try:\n",
    "            model = ARIMA(train_data['Quantity'], order=order).fit()\n",
    "            y_pred = model.predict(start=test_data.index[0], end=test_data.index[-1])\n",
    "            mape = MAPE(y_true, y_pred)\n",
    "            acc = Accuracy(mape)\n",
    "            rmse = RMSE(y_true, y_pred)\n",
    "            if rmse < best_rmse:\n",
    "                best_rmse = rmse\n",
    "                best_order = order\n",
    "                best_mape = mape\n",
    "                best_acc = acc\n",
    "                best_pred = y_pred\n",
    "        except:\n",
    "            pass\n",
    "    final_model = ARIMA(train_data['Quantity'], order = best_order).fit()\n",
    "    forecast = final_model.predict(start = testEndDate + timedelta(days=1), end = forecastEndDate)\n",
    "    temp = pd.DataFrame({'Part':np.repeat(part,n),\n",
    "                         'Date':test_data.index.values, \n",
    "                         'Original':test_data.Quantity.values, \n",
    "                         'Prediction':best_pred.values, \n",
    "                         'MAPE':np.repeat(best_mape,n), \n",
    "                         'RMSE':np.repeat(best_rmse,n), \n",
    "                         'Accuracy':np.repeat(best_acc,n)})\n",
    "    temp.insert(6, 'Order', [best_order for i in range(n)])\n",
    "    \n",
    "    forecast_df = pd.DataFrame({'Part':np.repeat(part,m),\n",
    "                         'Date':forecast_dates.values, \n",
    "                         'Forecast':forecast.values})\n",
    "                                \n",
    "    forecast_df.insert(3, 'Order', [best_order for i in range(m)])\n",
    "    return temp, forecast_df\n",
    "\n",
    "\n",
    "# ===========================================================================================================\n",
    "#                                  Auto Regressive Integrated Moving Average (ARIMA)\n",
    "# -----------------------------------------------------------------------------------------------------------\n",
    "def RUN_ARIMA(df, part):\n",
    "    best_rmse = float(\"inf\")\n",
    "    best_order = None\n",
    "    best_mape = None\n",
    "    best_acc = None\n",
    "    best_pred = None\n",
    "    # Getting data for each group\n",
    "    demo_data = data_part(df, part)\n",
    "    \n",
    "    # Data splitting\n",
    "    train_data = demo_data[:trainEndDate]\n",
    "    test_data = demo_data[trainEndDate + timedelta(days = 1):testEndDate]\n",
    "    forecast_dates = pd.date_range(testEndDate, forecastEndDate, freq = 'M') + timedelta(days = 1)\n",
    "    n = len(test_data)\n",
    "    m = len(forecast_dates)\n",
    "    y_true = test_data['Quantity']\n",
    "    \n",
    "    # Defining possible combination set\n",
    "    p = range(0, 10)\n",
    "    d = [adf_test(demo_data['Quantity'])]\n",
    "    q = range(0, 10)\n",
    "    pdq_combination = list(itertools.product(p,d,q))\n",
    "    \n",
    "    for pdq in tqdm(pdq_combination, desc = f'\\nPart: {part}', leave = False):\n",
    "        try:\n",
    "            model = ARIMA(train_data['Quantity'], order=pdq).fit()\n",
    "            y_pred = model.predict(start=test_data.index[0], end=test_data.index[-1])\n",
    "            mape = MAPE(y_true, y_pred)\n",
    "            acc = Accuracy(mape)\n",
    "            rmse = RMSE(y_true, y_pred)\n",
    "            if rmse < best_rmse:\n",
    "                best_rmse = rmse\n",
    "                best_order = pdq\n",
    "                best_mape = mape\n",
    "                best_acc = acc\n",
    "                best_pred = y_pred\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    # final model\n",
    "    ARIMA_final_model = ARIMA(train_data['Quantity'], order=best_order).fit()\n",
    "    forecast = ARIMA_final_model.predict(start = testEndDate + timedelta(days=1), end = forecastEndDate)\n",
    "    temp = pd.DataFrame({'Part':np.repeat(part,n), \n",
    "                         'Date':test_data.index.values, \n",
    "                         'Original':test_data.Quantity.values, \n",
    "                         'Prediction':best_pred.values, \n",
    "                         'MAPE':np.repeat(best_mape,n), \n",
    "                         'RMSE':np.repeat(best_rmse,n), \n",
    "                         'Accuracy':np.repeat(best_acc,n)})\n",
    "    temp.insert(6, 'Order', [best_order for i in range(n)])\n",
    "\n",
    "\n",
    "    forecast_df = pd.DataFrame({'Part':np.repeat(part,m), \n",
    "                         'Date':forecast_dates.values, \n",
    "                         'Forecast':forecast.values})\n",
    "    forecast_df.insert(3, 'Order', [best_order for i in range(m)])\n",
    "    return temp, forecast_df\n",
    "\n",
    "# ===========================================================================================================\n",
    "#                          Seasonal Auto Regressive Integrated Moving Average (SARIMA)\n",
    "# -----------------------------------------------------------------------------------------------------------\n",
    "def RUN_SARIMA(df, part):\n",
    "    best_rmse = float(\"inf\")\n",
    "    best_pdq = None\n",
    "    best_pdqs = None\n",
    "    best_mape = None\n",
    "    best_acc = None\n",
    "    best_pred = None\n",
    "\n",
    "    # Getting data for each group\n",
    "    demo_data = data_part(df, part)\n",
    "\n",
    "    # Data splitting\n",
    "    train_data = demo_data[:trainEndDate]\n",
    "    test_data = demo_data[trainEndDate + timedelta(days = 1):testEndDate]\n",
    "    forecast_dates = pd.date_range(testEndDate, forecastEndDate, freq = 'M') + timedelta(days = 1)\n",
    "    n = len(test_data)\n",
    "    m = len(forecast_dates)\n",
    "    y_true = test_data['Quantity']\n",
    "\n",
    "    #All possible combinations\n",
    "    p = range(0,3)\n",
    "    d = [adf_test(demo_data['Quantity'])]\n",
    "    q = range(0, 3)\n",
    "    P = range(0, 3)\n",
    "    D = [0,1]\n",
    "    Q = range(0, 3)\n",
    "    s = [3, 6, 12]\n",
    "    pdq_combination = list(itertools.product(p,d,q))\n",
    "    PDQs_combination = list(itertools.product(P, D, Q, s))\n",
    "    \n",
    "    for pdq in tqdm(pdq_combination, desc = f'\\nPart {part}', leave = False):\n",
    "        for PDQs in tqdm(PDQs_combination, desc = 'Searching for best combination', leave = False):\n",
    "            try:\n",
    "                model = SARIMAX(train_data['Quantity'], order = pdq, seasonal_order = PDQs).fit()\n",
    "                y_pred = model.predict(start = test_data.index[0], end = test_data.index[-1])\n",
    "                y_true = test_data['Quantity']\n",
    "                mape = MAPE(y_true, y_pred)\n",
    "                acc = Accuracy(mape)\n",
    "                rmse = RMSE(y_true, y_pred)\n",
    "                \n",
    "                if rmse < best_rmse:\n",
    "                    best_rmse = rmse\n",
    "                    best_mape = mape\n",
    "                    best_acc = acc\n",
    "                    best_pred = y_pred\n",
    "                    best_pdq = pdq\n",
    "                    best_pdqs = PDQs\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    temp = pd.DataFrame({'Part':np.repeat(part,n), \n",
    "                         'Date':test_data.index.values, \n",
    "                         'Original':test_data.Quantity.values, \n",
    "                         'Prediction':best_pred.values, \n",
    "                         'MAPE':np.repeat(best_mape,n), \n",
    "                         'RMSE':np.repeat(best_rmse,n), \n",
    "                         'Accuracy':np.repeat(best_acc,n)})\n",
    "    temp.insert(6, 'Order', [(best_pdq, best_pdqs) for i in range(n)])\n",
    "    \n",
    "    # Forecasting\n",
    "    final_model = SARIMAX(train_data['Quantity'], order = best_pdq, seasonal_order = best_pdqs).fit()\n",
    "    s = testEndDate + timedelta(days=1)\n",
    "    e = forecastEndDate\n",
    "    forecast = final_model.predict(start=s, end=e)\n",
    "    forecast_df = pd.DataFrame({'Part':np.repeat(part,m),\n",
    "                                \"Date\": forecast_dates.values ,\n",
    "                                \"Forecast\": forecast.values})\n",
    "    forecast_df.insert(3, 'Order', [(best_pdq, best_pdqs) for i in range(m)])\n",
    "    \n",
    "    return temp, forecast_df\n",
    "\n",
    "# ===========================================================================================================\n",
    "#                                                   TBATS\n",
    "# -----------------------------------------------------------------------------------------------------------\n",
    "def RUN_TBATS(df, part):\n",
    "    best_rmse = float(\"inf\")\n",
    "    best_sp = None\n",
    "    best_mape = None\n",
    "    best_acc = None\n",
    "    best_pred = None\n",
    "\n",
    "    # Getting data for each group\n",
    "    demo_data = data_part(df, part)\n",
    "    sp_values = [3, 4, 5, 6, 8, 9, 12]\n",
    "\n",
    "    # Data splitting\n",
    "    train_data = demo_data[:trainEndDate]\n",
    "    test_data = demo_data[trainEndDate + timedelta(days = 1):testEndDate]\n",
    "    forecast_dates = pd.date_range(forecastStartDate - timedelta(days = 1), forecastEndDate, freq='M') + timedelta(days = 1)\n",
    "    n = len(test_data)\n",
    "    m = len(forecast_dates)\n",
    "    y_true = test_data['Quantity']\n",
    "    \n",
    "    for sp in tqdm(sp_values, desc = 'Searching over possible seasonal periods', leave = False):\n",
    "        try:\n",
    "            model = TBATS(seasonal_periods=[sp]).fit(train_data.Quantity)\n",
    "            y_pred = model.forecast(steps=n)\n",
    "            mape = MAPE(y_true, y_pred)\n",
    "            acc = Accuracy(mape)\n",
    "            rmse = RMSE(y_true, y_pred)\n",
    "            if rmse < best_rmse:\n",
    "                best_rmse = rmse\n",
    "                best_sp = sp\n",
    "                best_mape = mape\n",
    "                best_acc = acc\n",
    "                best_pred = y_pred\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    final_model = TBATS(seasonal_periods=[best_sp]).fit(train_data.Quantity) \n",
    "    y_forecast = final_model.forecast(steps=12+m)\n",
    "    \n",
    "    temp_df = pd.DataFrame({'Part':np.repeat(part,n), \n",
    "                         'Date':test_data.index.values, \n",
    "                         'Original':test_data.Quantity.values, \n",
    "                         'Prediction':[best_pred[i] for i in range(0, n)], \n",
    "                         'MAPE':np.repeat(best_mape,n), \n",
    "                         'RMSE':np.repeat(best_rmse,n), \n",
    "                         'Accuracy':np.repeat(best_acc,n)})\n",
    "    temp_df.insert(6, 'Order', [(best_sp) for i in range(n)])\n",
    "    \n",
    "    forecast_df = pd.DataFrame({'Part':np.repeat(part, m), \n",
    "                         'Date':forecast_dates.values, \n",
    "                         'Forecast':[y_forecast[n-1+i] for i in range(0, m)]})\n",
    "    forecast_df.insert(3, 'Order', [(best_sp) for i in range(m)])\n",
    "    \n",
    "    return temp_df, forecast_df\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "#                                   Prophet\n",
    "# ----------------------------------------------------------------------------\n",
    "def RUN_PROPHET(df, part):\n",
    "    rmse = float(\"inf\")\n",
    "    mape = None\n",
    "    acc = None\n",
    "\n",
    "    # Getting data for each group\n",
    "    demo_data = data_part(df, part)\n",
    "    demo_data.reset_index(level=0, inplace=True)\n",
    "    demo_data.columns = ['Date', 'Quantity', 'Flag NA', 'Outlier']\n",
    "\n",
    "    # Data splitting\n",
    "    testEndDate = demo_data.index.values[-1]\n",
    "    train_data = demo_data.iloc[:demo_data.index[demo_data.Date == trainEndDate].values[0]]\n",
    "    test_data = demo_data.iloc[demo_data.index[demo_data.Date == trainEndDate].values[0]+1:testEndDate]\n",
    "    train_data = train_data[[\"Date\", \"Quantity\"]]\n",
    "    train_data.columns = ['ds', 'y']\n",
    "    test_period = pd.DataFrame(test_data[\"Date\"])\n",
    "    test_period.columns = ['ds']\n",
    "    y_true = test_data['Quantity']\n",
    "    n = len(test_data)\n",
    "    forecast_dates = pd.date_range(forecastStartDate - timedelta(days = 1), forecastEndDate , freq='M') + timedelta(days = 1)\n",
    "    \n",
    "    try:\n",
    "        model = Prophet()\n",
    "        model.fit(train_data)\n",
    "        prediction = model.predict(test_period)\n",
    "        y_pred = prediction[\"yhat\"]\n",
    "        mape = MAPE(y_true, y_pred)\n",
    "        acc = Accuracy(mape)\n",
    "        rmse = RMSE(y_true, y_pred)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    forecast_period = pd.DataFrame(forecast_dates)\n",
    "    forecast_period.columns = ['ds']\n",
    "    m = len(forecast_period)\n",
    "    forecast = model.predict(forecast_period)\n",
    "    y_forecast = forecast[\"yhat\"]\n",
    "    \n",
    "    temp_df = pd.DataFrame({'Part':np.repeat(part,n), \n",
    "                         'Date':test_data.Date.values, \n",
    "                         'Original':test_data.Quantity.values, \n",
    "                         'Prediction':[y_pred[i] for i in range(0, n)], \n",
    "                         'MAPE':np.repeat(mape,n), \n",
    "                         'RMSE':np.repeat(rmse,n), \n",
    "                         'Accuracy':np.repeat(acc,n)})\n",
    "    temp_df.insert(6, 'Order', [np.nan for i in range(n)])\n",
    "    \n",
    "    forecast_df = pd.DataFrame({'Part':np.repeat(part,m), \n",
    "                         'Date':forecast_period.ds.values, \n",
    "                         'Forecast':[y_forecast[i] for i in range(0, m)]})\n",
    "    forecast_df.insert(3, 'Order', [np.nan for i in range(m)])\n",
    "    \n",
    "    return temp_df, forecast_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RUN_MODEL(date_format, data_file_name, model, model_fun):\n",
    "    # Load the data and data processing ------------------------------------------\n",
    "    data = pd.read_csv(f'./../data/{data_file_name}.csv', low_memory = False, encoding= 'unicode_escape')\n",
    "    df = data[['Part','Customer Desc','Date','Quantity']]\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], format = date_format)\n",
    "    # Define SKU list ------------------------------------------------------------\n",
    "    sku_set = df['Part'].unique()[:2]\n",
    "    # Run all SKU for a specific model -------------------------------------------\n",
    "    print(f'MODEL: {model} || Running for {len(sku_set)} parts: ')\n",
    "    prediction_df = pd.DataFrame()\n",
    "    forecast_df = pd.DataFrame()\n",
    "    for part in tqdm(sku_set, desc = 'Over All Parts'):\n",
    "        output = model_fun(df, part)\n",
    "        prediction_df = pd.concat([prediction_df, output[0]], ignore_index = True)\n",
    "        forecast_df = pd.concat([forecast_df, output[1]], ignore_index = True)\n",
    "\n",
    "    summary_df = prediction_df[['Part','MAPE','RMSE','Accuracy','Order']]\n",
    "    summary_df.drop_duplicates(inplace = True)\n",
    "    summary_df.reset_index(drop = True, inplace = True)\n",
    "    summary_df['Model'] = np.repeat('Model', len(sku_set))\n",
    "\n",
    "    prediction_df.to_csv(f'./../result/prediction_{model}_{len(sku_set)}.csv', index = False)\n",
    "    forecast_df.to_csv(f'./../result/forecast_{model}_{len(sku_set)}.csv', index = False)\n",
    "    summary_df.to_csv(f'./../result/summary_{model}_{len(sku_set)}.csv', index = False)\n",
    "\n",
    "    print(f\"Average MAPE for {model}: {summary_df['MAPE'].mean()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally Run All model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: PROBHET || Running for 2 parts: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a511d6221f73404e95218bf83e4220ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Over All Parts'), FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average MAPE for PROBHET: 20.695249800231757\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function input\n",
    "# -----------------------------------------------------------------------------\n",
    "date_format = '%d-%b-%y'\n",
    "data_file_name = ''\n",
    "\n",
    "RUN_MODEL(date_format, data_file_name, 'HW', RUN_HW)\n",
    "RUN_MODEL(date_format, data_file_name, 'ARMA', RUN_ARMA)\n",
    "RUN_MODEL(date_format, data_file_name, 'ARIMA', RUN_ARIMA)\n",
    "RUN_MODEL(date_format, data_file_name, 'SARIMA', RUN_SARIMA)\n",
    "RUN_MODEL(date_format, data_file_name, 'TBATS', RUN_TBATS)\n",
    "RUN_MODEL(date_format, data_file_name, 'PROBHET', RUN_PROPHET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define lists\n",
    "# models = ['HW','ARMA','ARIMA','SARIMA','PROPHET','TBATS']\n",
    "\n",
    "# # Define function to concate model wise table\n",
    "# def concat_df(paths, model_name):\n",
    "#     output = pd.DataFrame()\n",
    "#     for path in paths:\n",
    "#         temp = pd.concat([temp, pd.read_csv(path)])\n",
    "#     output.reset_index(drop = True, inplace = True)\n",
    "#     temp['Model'] = model_name\n",
    "#     return temp\n",
    "\n",
    "# # Collecting files paths\n",
    "# path_files = []\n",
    "# for model in models:\n",
    "#     for part in parts:\n",
    "#         path_files.append('./../result/results_' + model + str(part) + '.csv')\n",
    "\n",
    "# # Read all those files pair-wise\n",
    "# p = len(parts)\n",
    "# hw = concat_df(path_files[0:p], 'HW')\n",
    "# arma = concat_df(path_files[p:2*p], 'ARMA')\n",
    "# arima = concat_df(path_files[2*p:3*p], 'ARIMA')\n",
    "# sarima = concat_df(path_files[3*p:4*p], 'SARIMA')\n",
    "# prophet = concat_df(path_files[4*p:5*p], 'PROPHET')\n",
    "# tbats = concat_df(path_files[5*p:6*p], 'TBATS')\n",
    "\n",
    "# # Let's concate everything\n",
    "# results_all_model = pd.concat([hw, arma, arima, sarima, prophet, tbats])\n",
    "# results_all_model.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# # Assigning volatility and priority\n",
    "# volatile = []\n",
    "# priority = []\n",
    "# for part in results_all_model['Part']:\n",
    "#     volatile.append(assign_volatility(part))\n",
    "#     priority.append(assign_priority(part))\n",
    "# results_all_model['Volatility'] = volatile\n",
    "# results_all_model['Priority'] = priority\n",
    "\n",
    "# # Store the file\n",
    "# print(f'Shape: {results_all_model.shape} and shape should be ({sum(parts)*12*6}, {11})')\n",
    "# results_all_model.to_csv('./../result/final_merged_prediction_305.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
